<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Conditional Expectation is Just Projection! – Emma Finn</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- Tailwind (optional if you already include it globally) -->
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.17/dist/tailwind.min.css" rel="stylesheet" />

  <!-- KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body,{
      delimiters:[
        {left:'$$', right:'$$', display:true},
        {left:'$', right:'$', display:false},
        {left:'\\(', right:'\\)', display:false},
        {left:'\\[', right:'\\]', display:true}
      ],
      throwOnError:false
    });"
  ></script>
</head>
<body class="bg-gray-50 text-gray-800 leading-relaxed">
  <header class="bg-white shadow">
    <div class="container mx-auto px-4 py-4 flex items-center justify-between">
      <a href="index.html" class="text-blue-600 hover:underline">← Back to Blog</a>
      <h1 class="text-2xl font-bold">Conditional Expectation is Just Projection!</h1>
    </div>
  </header>

  <main class="container mx-auto px-4 py-8">
    <div class="text-gray-600 text-sm italic mb-8">
      July 2025 • Emma Finn
    </div>

      <figure class="max-w-2xl mx-auto">
    <img
      src="figure1post1.jpg"
      alt="Pam from The Office saying Gram–Schmidt and Variance Decomposition Sobol's Method are the same picture"
      loading="lazy"
      class="w-full rounded shadow"
    />
    <figcaption class="mt-2 text-center text-sm text-gray-600">
      <span class="font-semibold">Figure&nbsp;1.</span>
      Gram–Schmidt vs. Sobol’s method: corporate can’t tell the difference.
    </figcaption>
  </figure>
</section>

    <!-- What is Sobol's Method  -->
    <section id="tldr" class="mb-10">
      <h2 class="text-xl font-semibold mb-3">TL;DR</h2>
      <p>
        Suppose we have a model $f:\mathbb{R}^n \to \mathbb{R}$ that takes a vector of $n$ inputs. You might imagine our model
        is a classifier that operates on MNIST and outputs the probability that the image is an $8$. It takes in a vector of inputs corresponding to patches in a black-and-white image of 
        a hand-drawn digit. A natural question is what percentage of the variance in the probability of an image being an eight can be
        attributed to a particular patch. This is the question that Sobol's method answers.
      </p>
    </section>

    <!-- Body -->
    <section id="setup" class="mb-10 space-y-4">
      <h2 class="text-xl font-semibold mb-3">1. The Setup</h2>
      <p>
       We assume that our inputs are independent and distributed uniformly at random. 
        Without loss of generality, let's assume that our inputs live on the $ n$-dimensional hypercube.
       We decompose $f$ into orthogonal components—single–variable effects, pairwise interactions. 
      Each component is obtained by projecting $f$ onto the subspace of
      interaction functions depending only on a specified subset of inputs. 
        This mirrors the Gram–Schmidt process in
        the Hilbert space $L_2$, where conditional expectations play the role of orthogonal projections. In the end,
        the variance of $Y=f(X)$ splits into uncorrelated pieces attributable to individual variables and their
        Let $X=(X_1,\dots,X_n)$ be inputs and $Y=f(X)$ the scalar output. Assume $f\in L_2$, i.e.,
        $\int f^2(x)\,dx &lt; \infty$, so we can work in the Hilbert space
        $L_2$ with inner product $\langle U,V\rangle = \mathbb{E}[UV]$.
      </p>
      <p>
        We expand $f$ as a sum of orthogonal components:
      </p>
      <p class="overflow-x-auto">
        $$f(X_1,\dots,X_n) = f_0 + \sum_{i=1}^n f_i(X_i) + \sum_{i&lt;j} f_{ij}(X_i,X_j)
        + \sum_{i&lt;j&lt;k} f_{ijk}(X_i,X_j,X_k) + \cdots + f_{1\,2\,\dots n}(X_1,\dots,X_n).$$
      </p>
    </section>

    <section id="components" class="mb-10 space-y-4">
      <h2 class="text-xl font-semibold mb-3">2. How We Get the Components</h2>
      <p>
        Start with the mean term:
      </p>
      <p class="overflow-x-auto">$$f_0 = \mathbb{E}[Y].$$</p>

      <p>
        The first–order (main effect) component for $X_i$ is
      </p>
      <p class="overflow-x-auto">
        $$f_i(X_i) = \mathbb{E}[Y\mid X_i] - f_0.$$
      </p>

      <p>
        The second–order interaction between $X_i$ and $X_j$ is
      </p>
      <p class="overflow-x-auto">
        $$f_{ij}(X_i,X_j) =
          \mathbb{E}[Y\mid X_i,X_j]
          - f_i(X_i) - f_j(X_j) - f_0.$$
      </p>

      <p>
        And so on: each higher‑order term subtracts all lower‑order pieces so that every component has
        mean zero and is orthogonal to all lower‑order components. (By the law of total expectation—often
        jokingly called “Adam’s law”—each $f_{S}$, for $S\subseteq\{1,\dots,n\}$ with $|S|\ge 1$, satisfies
        $\mathbb{E}[f_S]=0$.)
      </p>
    </section>

    <section id="projection" class="mb-10 space-y-4">
      <h2 class="text-xl font-semibold mb-3">3. Conditional Expectation as Projection</h2>
      <p>
        Think of all functions of $X_i$ as a subspace of $L_2$. The conditional expectation $\mathbb{E}[Y\mid X_i]$
        is the orthogonal projection of $Y$ onto that subspace. Therefore
      </p>
      <p class="overflow-x-auto">
        $$f_i = \mathbb{E}[Y\mid X_i] - \mathbb{E}[Y]$$
      </p>
      <p>
        is orthogonal to the constant subspace (spanned by $f_0$). This is directly analogous to the
        Gram–Schmidt process: each time we subtract projections onto already–constructed orthogonal components,
        producing a new piece that is orthogonal to all previous ones. The sum of all components still equals
        $f(X)$ exactly—no information is lost.
      </p>
    </section>

    <section id="variance" class="mb-10 space-y-4">
      <h2 class="text-xl font-semibold mb-3">4. Decomposing the Variance</h2>
      <p>
        Orthogonality implies uncorrelatedness. Squaring and integrating (i.e., taking $L_2$ norms),
        we obtain a variance decomposition. Write the index set of inputs as $[n]=\{1,\dots,n\}$ and let
        $S\subseteq [n]$ denote a subset. Then
      </p>
      <p class="overflow-x-auto">
        $$\operatorname{Var}(Y) = \sum_{\emptyset \neq S \subseteq [n]} V_S,$$
      </p>
      <p>
        where, for singletons,
      </p>
      <p class="overflow-x-auto">
        $$V_i = \operatorname{Var}_{X_i}\big( \mathbb{E}[Y \mid X_i] \big),$$
      </p>
      <p>
        and for pairs,
      </p>
      <p class="overflow-x-auto">
        $$V_{ij} = \operatorname{Var}_{X_i,X_j}\big( \mathbb{E}[Y \mid X_i, X_j] \big) - V_i - V_j,$$
      </p>
      <p>
        and, in general,
      </p>
      <p class="overflow-x-auto">
        $$V_S = \operatorname{Var}_{X_S}\big( \mathbb{E}[Y \mid X_S] \big) - \sum_{T \subsetneq S} V_T.$$
      </p>
      <p>
        Equivalently, because the $f_S$’s are orthogonal,
      </p>
      <p class="overflow-x-auto">
        $$\operatorname{Var}(Y) = \sum_{\emptyset\neq S\subseteq [n]} \|f_S\|_{L_2}^2
          = \sum_{s=1}^n \sum_{i_1&lt;\cdots&lt;i_s} \int f_{i_1\cdots i_s}^2(x_{i_1},\dots,x_{i_s})\,dx_{i_1}\cdots dx_{i_s}.$$
      </p>
      <p>
        This gives a clean way to attribute the total variance of the model to individual inputs and their
        interactions. Pretty amazing!
      </p>
    </section>

    <!-- (Optional) Manim animation slot -->
    <section id="animation" class="mb-12">
      <h2 class="text-xl font-semibold mb-3">Visualizing the Projections</h2>
      <video controls loop muted class="w-full rounded shadow">
        <source src="/media/videos/ProjectionDemo.webm" type="video/webm" />
        Your browser does not support embedded videos.
      </video>
    </section>

    <section id="references" class="border-t pt-6 text-sm text-gray-600">
      <h2 class="font-semibold mb-2">References &amp; Further Reading</h2>
      <ul class="list-disc list-inside space-y-1">
        <li>Sobol, I. M. (1993). Sensitivity estimates for nonlinear mathematical models.</li>
        <li>Saltelli et al. (2008). <em>Global Sensitivity Analysis: The Primer</em>.</li>
        <li>Any standard text on $L_2$ spaces, conditional expectation, and Gram–Schmidt.</li>
      </ul>
    </section>
  </main>

  <footer class="bg-white border-t">
    <div class="container mx-auto px-4 py-4 text-center text-gray-600 text-sm">
      © 2025 Emma Finn. All rights reserved.
    </div>
  </footer>
</body>
</html>
